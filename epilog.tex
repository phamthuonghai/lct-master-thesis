\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}

Furthermore, leveraging the self-attention weights to do both parsing and translation outperformed the baseline translation model and was competitive to referential parsers. However, our model was trained on a synthetic dataset in case of parsing. Hence, it is a good practice to fine-tune our model with the gold annotated trees, which we believe should lead to a better parsing performance, yet we leave it for future work.

We further invented a novel component of Transformer model for dependency parsing by promoting the dependency interpretation of self-attention, and showed that Transformer model can be used as very effective and precise parser, aiming at beating state-of-the-art, while not loosing ability to translate.
