\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}

Neural Machine Translation (NMT) has been lately established state of the art in machine translation (MT). To achieve this result, most NMT models highly rely on the attention mechanism \citep{bahdanau:etal:attention:iclr:2015}. It was claimed that this attention mechanism can capture some linguistic structures and phenomena \citep{DBLP:conf/nips/VaswaniSPUJGKP17}. Although \cite{DBLP:conf/acl/BelinkovDDSG17} and \cite{DBLP:conf/emnlp/ShiPK16} had analyzed the amount of linguistic information, POS tags and syntax, respectively, NMT systems could capture, their analyses only applied to systems without attention mechanism. More notably, \cite{DBLP:conf/ijcnlp/GhaderM17} had examined the similarities and differences between attention and alignment matrix. However, their goal was merely to inspect the alignment, not linguistic knowledge in source sentence nor target sentences. Therefore, this thesis is aimed to examine the relevance between linguistic structures, e.g. dependency syntax \citep{melvcuk1988dependency}, and the (self-)attention mechanism (within sentence/self-attention) in NMT, by focusing on two basic questions:
\begin{itemize}
	\item Does explicitly feeding dependency parse to an NMT system help?
    \item If not, is the attention mechanism able to capture these information?
\end{itemize}
